# Module 14: Code Generation

## Overview

Understand how LLMs generate code, how to evaluate them, and how coding agents like SWE-Agent work.

## Learning Objectives

By the end of this module, you should be able to:

1. Evaluate code generation quality using standard benchmarks
2. Understand SWE-Bench and why it matters
3. Explain how coding agents navigate codebases
4. Build or extend a simple coding agent

## Key Concepts

- **HumanEval**: Function completion benchmark
- **SWE-Bench**: Real GitHub issue resolution
- **The Stack**: Large-scale code training data
- **Flow Engineering**: Multi-step code generation (AlphaCodeium)
- **Coding Agents**: Autonomous code modification systems

## Prerequisites

- Completed Module 13 (Agents & Tool Use)
- Comfortable reading and writing code
- Basic understanding of software development workflows

## Time Estimate

- Readings: 6-8 hours
- Hands-on: 8-10 hours
- Integrated into Project 5

## Why This Matters

Code generation is where LLMs have the most immediate practical impact. Understanding:
- How to evaluate code models properly
- What makes coding agents work (and fail)
- The gap between benchmarks and real-world performance

See [readings.md](readings.md) for the full reading list.
